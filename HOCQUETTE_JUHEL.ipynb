{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e804e309",
   "metadata": {},
   "source": [
    "Reinforcement Learning Exam\n",
    "\n",
    "- Year: 2024-2025\n",
    "- Instructor: Richard Combes\n",
    "- Email: richard.combes@centralesupelec.fr\n",
    "- Deadline: 28th febuary at 23h59 (Paris time)\n",
    "\n",
    "\n",
    "Instructions:\n",
    "- the exam is to be done by groups of 2 students\n",
    "- complete this notebook with your answers\n",
    "- your answers will include both the python code as well as figures and explanations in text\n",
    "- when you are done, send this notebook by email to the instructor\n",
    "- to avoid confusion use the following naming convention for your notebook file: \"(name of first student)(name of second student).ipynb\"\n",
    "- any exam received after the deadline will not be considered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06439f95",
   "metadata": {},
   "source": [
    "Introduction and Model\n",
    "\n",
    "The goal of this exam is to learn how to drive a car up a hill using reinforcement learning. To do so we will consider the following model.\n",
    "\n",
    "We assume that the car is modelled by a point, located at location $(x,z)$ where $x$ and $z$ denote the coordinates of the car on the $x$ axis and the $z$ axis respectively. The car is located on a hill, so that $z = h(x)$ where $h(x)$ denotes the height of the hill at location $x$ given by\n",
    "$$\n",
    "h(x) = {1 \\over 2} x^2\n",
    "$$\n",
    "\n",
    "Location $(x,z) = (0,0)$ is the bottom of the hill, and location $x=(1,{1 \\over 2})$ is the top of the hill. \n",
    "\n",
    "The goal is for the driver of the car to get the car to the top of the hill as fast as possible, and we assume that once the top of the hill has been reached the experiment stops. An illustration is found below, where the car is represented as a red dot and the target is represented as a green dot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e73f8275",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import display, HTML\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306fd58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "deltax = 0.01\n",
    "x_vector = np.arange(-1,1+deltax,deltax)\n",
    "z_vector = 0.5*(x_vector) ** 2\n",
    "x_0 = 0\n",
    "z_0 = 0\n",
    "plt.figure()\n",
    "plt.plot(x_vector, z_vector)\n",
    "plt.plot(x_0, z_0,'ro')\n",
    "plt.plot(1, 0.5,'go')\n",
    "plt.xlabel('x coordinate')\n",
    "plt.ylabel('z coordinate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e9af4",
   "metadata": {},
   "source": [
    "Control Strategy\n",
    "\n",
    "To control the car, the driver acts on the engine and its action is modelled by $w \\in \\{-1,0,1\\}$ where $w=0$ means that the engine is inactive, $w=1$ means that the driver is accelerating the car forward as hard as possible, and $w=-1$ means that the driver is accelerating the car backwards as hard as possible.\n",
    "\n",
    "The trajectory of the car is determined by Newton's law which is the second order differential equation:\n",
    "$$\n",
    "    m {d^2  x \\over d t^2}(t) =  {1 \\over \\sqrt{1 + (h'(x(t)))^2}} \\Big[ -g m {h'(x(t)) \\over \\sqrt{1 + (h'(x(t)))^2}} + K m w(t)\\Big]\n",
    "$$\n",
    "with $x(t)$ the location of the car at time $t$, $w(t)$ the action of the driver at time $t$, $m$ the mass of the car, $g$ the gravitational constant and $K$ proportional to the maximal torque of the engine.\n",
    "\n",
    "We assume that the car starts at the bottom of the hill from a dead stop, so that \n",
    "$$\n",
    "\\Big(x(0),{d  x \\over d t}(0)\\Big)  = (0,0)\n",
    "$$\n",
    "\n",
    "The differential equation simplifies to:\n",
    "$$\n",
    "    {d^2  x \\over d t^2}(t) =  - {g x(t) \\over 1 + x(t)^2} +  {K w(t) \\over \\sqrt{1 + x(t)^2}}\n",
    "$$\n",
    "and we assume that $g=10$ and $K=1$ throughout.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578424c",
   "metadata": {},
   "source": [
    "Casting the problem as a Markov Decision Process\n",
    "\n",
    "In order to fit the framework seen in class, we consider a discretized time version that can be written as a Markov Decision Process. The state of the system at time instants $k=0,1,...$ is given by its position and speed at time instants $t = k \\Delta$ for $k$ integer:\n",
    "$$\n",
    "s_k = \\Big(s_{k,0},s_{k,1}\\Big) = \\Big( x(k \\Delta) , {d x \\over d t}(k \\Delta) \\Big)\n",
    "$$\n",
    "where $\\Delta$ is a small discretization step. It is noted that the state contains both the position and the speed of the car, which is necessary.\n",
    "\n",
    "The action at time instant $k$ is given by\n",
    "$$\n",
    "a_k = w(k \\Delta t) \\in \\{-1,0,+1\\}   \n",
    "$$\n",
    "The reward is given by\n",
    "$$\n",
    "r_k = 1( |s_{k,0}| \\ge 1 )\n",
    "$$\n",
    "so that the reward is $1$ if the top of the hill has been reached and $0$ otherwise.\n",
    "\n",
    "One can readily check that this defines a Markov Decision Process. \n",
    "\n",
    "The evolution of the state $s$ given the action $a$ is given by discretizing the differential equation above:\n",
    "$$\n",
    "    {s_{k+1,0} - s_{k,0} \\over \\Delta} =  s_{k,1} \\text{     and       } {s_{k+1,1}-s_{k,1} \\over \\Delta}   =  { g s_{k,0} \\over 1 + (s_{k,0})^2 }  +  {K a_k \\over \\sqrt{1 + (s_{k,0})^2} }\n",
    "$$\n",
    "\n",
    "We will consider $\\Delta = 0.01$ throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c225908a",
   "metadata": {},
   "source": [
    "Getting Started\n",
    "\n",
    "In order to get started you are provided with several pieces of code:\n",
    "- the \"next_state\" subroutine takes as input the current state $s_k$ and the current action $a_k$ and outputs the next state $s_{k+1}$\n",
    "- the \"policy1\" subroutine is an example of a policy. We recall that a policy takes the current state $s_k$ as an input and outputs the current action $a_k$. Here this policy is a simple policy where $a_k=1$ irrespective of $s_k$, meaning that the driver always accelerates the car forward.\n",
    "- the \"generate_episode\" subroutine takes as an input a time horizon $T$ and a policy, and outputs the trajectory of the system $(s_{k})_{k=1,...,T}$.\n",
    "- the last piece of code generates an animation that represents the trajectory of the system under the policy described above\n",
    "\n",
    "You can reuse and modify those pieces of code in order to answer the exam questions.\n",
    "\n",
    "Note: some policies might take a large, possibly infinite, amount of time in order to reach the top of the hill, therefore you should simulate episodes up to a well chosen maximal duration $T_{max}$, in order to avoid infinite loops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c45b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state(s,a):\n",
    "    delta = 0.01\n",
    "    g = 10.0\n",
    "    K = 1.0\n",
    "    snext = list(s)\n",
    "    snext[0] = s[0] + s[1]*delta\n",
    "    snext[1] = s[1] + (-g*s[0]/(1 + (s[0])**2) + K*a/np.sqrt(1 + (s[0])**2))*delta\n",
    "    return snext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a9a4087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy1(s):\n",
    "    a = 1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "686161cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(T,policy):\n",
    "    s_t = []\n",
    "    s = [0.0,0.0]\n",
    "    for t in range(T):\n",
    "        s_t.append(list(s))\n",
    "        a = policy(s)\n",
    "        s = next_state(s,a)\n",
    "    return s_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17cc069a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"%matplotlib notebook\\n\\n\\nT = 1000\\ns_t = generate_episode(T,policy1)\\nfig, ax = plt.subplots()\\nline, = plt.plot(-1, 1,'ro')\\nline1, = plt.plot(1, 0.5,'go')\\nline2, = plt.plot(x_vector, z_vector,'b')\\nax.set_xlim(-1.1,1.1)\\nax.set_ylim(0, 0.6)\\ndef animate(t):\\n    s = s_t[t]\\n    line.set_data((s[0],0.5*s[0]**2))\\n    return line\\nanim = FuncAnimation(fig, animate, frames=(T-1), interval=1000)\\nplt.show()\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"%matplotlib notebook\n",
    "\n",
    "\n",
    "T = 1000\n",
    "s_t = generate_episode(T,policy1)\n",
    "fig, ax = plt.subplots()\n",
    "line, = plt.plot(-1, 1,'ro')\n",
    "line1, = plt.plot(1, 0.5,'go')\n",
    "line2, = plt.plot(x_vector, z_vector,'b')\n",
    "ax.set_xlim(-1.1,1.1)\n",
    "ax.set_ylim(0, 0.6)\n",
    "def animate(t):\n",
    "    s = s_t[t]\n",
    "    line.set_data((s[0],0.5*s[0]**2))\n",
    "    return line\n",
    "anim = FuncAnimation(fig, animate, frames=(T-1), interval=1000)\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17ba9a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next available GIF path: ./gifs\\animation_13.gif\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_next_gif_path(folder: str, base_name: str = \"animation\") -> str:\n",
    "    \"\"\"\n",
    "    Determines the next available file path for a new GIF in the given folder.\n",
    "    \n",
    "    :param folder: The directory containing the GIF files.\n",
    "    :param base_name: The base name for the GIF files (default is \"animation\").\n",
    "    :return: The next available file path as a string.\n",
    "    \"\"\"\n",
    "    existing_numbers = []\n",
    "    \n",
    "    # Ensure the folder exists\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    # List files in the directory\n",
    "    for file in os.listdir(folder):\n",
    "        if file.startswith(base_name) and file.endswith(\".gif\"):\n",
    "            try:\n",
    "                number = int(file[len(base_name) + 1:-4])  # Extract the number\n",
    "                existing_numbers.append(number)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    next_number = max(existing_numbers, default=0) + 1\n",
    "    return os.path.join(folder, f\"{base_name}_{next_number}.gif\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"./gifs\"\n",
    "new_gif_path = get_next_gif_path(folder_path)\n",
    "print(\"Next available GIF path:\", new_gif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f78d55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The car reached the top of the hill at t=4793.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./gifs/animation_9.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def simulate_policy(policy, T=int(1e4), show=False):\n",
    "    s_t = generate_episode(T, policy)\n",
    "    if show:\n",
    "        fig, ax = plt.subplots()\n",
    "        line, = plt.plot(0, 0, 'ro')\n",
    "        line1, = plt.plot(1, 0.5, 'go')\n",
    "        line2, = plt.plot(x_vector, z_vector, 'b')\n",
    "        ax.set_xlim(-1.1,1.1)\n",
    "        ax.set_ylim(0, 0.6)\n",
    "\n",
    "    def animate(t):\n",
    "        s = s_t[t]\n",
    "        line.set_data([s[0]], [0.5*s[0]**2])\n",
    "        return line\n",
    "    \n",
    "    success_t = 0\n",
    "    for t in range(T):\n",
    "        if abs(s_t[t][0] - 1) < 1e-2 or s_t[t][0] >= 1:\n",
    "            success_t = t\n",
    "            break\n",
    "\n",
    "    if success_t != 0:\n",
    "        print(f\"The car reached the top of the hill at t={success_t}.\")\n",
    "    else:\n",
    "        print(f\"The car did not reach the top of the hill after t_max={T}.\")\n",
    "\n",
    "    if show:\n",
    "        gif_path = get_next_gif_path(\"./gifs\")\n",
    "        if os.path.exists(gif_path):  # Check if the file exists\n",
    "            os.remove(gif_path)       # Delete the file\n",
    "            print(f\"{gif_path} has been deleted.\")\n",
    "        else:\n",
    "            print(f\"{gif_path} does not exist.\")\n",
    "        anim = FuncAnimation(fig, animate, frames=range(0, T, 10), interval=5000 / (10*T))\n",
    "        anim.save(gif_path, writer=\"pillow\")\n",
    "        display(HTML(f'<img src=\"{gif_path}\">'))\n",
    "\n",
    "        plt.close(fig)\n",
    "\n",
    "simulate_policy(policy1, T=5000, show=False)\n",
    "\n",
    "this_gif = \"./gifs/animation_9.gif\"\n",
    "display(HTML(f'<img src=\"{this_gif}\">'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c2744b",
   "metadata": {},
   "source": [
    "___\n",
    "Question 1. Observe and comment the result of the simple policy described above where $a_k=1$ for all $k$. What is the amount of time required for the car to reach the top of the hill ? Is this policy optimal, and if not, can you suggest a better policy ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82efff48",
   "metadata": {},
   "source": [
    "**Answer:** The car reaches the top of the hill after 4793 states. In this policy, the car always accelerates. The problem is that the absolute speed decreases when $s_{k,0} \\leq 0$ and $a_k = 1$. A better policy is to take advantage of the states where $s_{k,0} \\leq 0$ to set $a_k = -1$ and get even more speed.\n",
    "\n",
    "So the policy would be:\n",
    "- $a_k=1$ when $s_{k,1} \\geq 0$\n",
    "- $a_k=-1$ when $s_{k,1} < 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ea537",
   "metadata": {},
   "source": [
    "___\n",
    "Question 2. Consider three policies: \n",
    "- the policy where $a_k = 1$ when $s_{k,0} \\ge 0$, and $a_k = -1$ when $s_{k,0} < 0$ \n",
    "- the policy where $a_k$ is i.i.d. uniformly distributed in $\\{-1,0,1\\}$\n",
    "- the policy where $a_k = 1$ when $s_{k,0} \\ge 0$, and $a_k = 0$ when $s_{k,0} < 0$ \n",
    "\n",
    "For each of those three policies, simulate the trajectory of the system and comment on the time necessary for the car to reach the top of the hill.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fee3739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The car reached the top of the hill at t=5869.\n",
      "./gifs\\animation_13.gif does not exist.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./gifs\\animation_13.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### FIRST POLICY ###\n",
    "def policy_Q2_1(s):\n",
    "    a = 1 if s[0] >= 0 else -1\n",
    "    return a\n",
    "\n",
    "simulate_policy(policy_Q2_1, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1416f63",
   "metadata": {},
   "source": [
    "As we can see, with the first policy, the time to reach the top of the hill is 5869, which is longer than the previous simulation. It surprises me. Actually, we notice that the car spends more time on the $s_{k,0}\\leq 0$ area so it may take it longer to reach the top, even it it reaches it with more speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1cde1abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The car reached the top of the hill at t=6629.\n",
      "./gifs\\animation_11.gif does not exist.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./gifs\\animation_11.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### SECOND POLICY ###\n",
    "def policy_Q2_2(s):\n",
    "    a = np.random.choice([-1, 0, 1])\n",
    "    return a\n",
    "\n",
    "simulate_policy(policy_Q2_2, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c8e70",
   "metadata": {},
   "source": [
    "This policy is clearly not optimal. We observe that the driver takes random decisions and it takes often more time to reach the top, even though for some tries, the car reaches the top faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc7f736f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The car reached the top of the hill at t=5697.\n",
      "./gifs\\animation_12.gif does not exist.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./gifs\\animation_12.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### THIRD POLICY ###\n",
    "def policy_Q2_3(s):\n",
    "    a = 1 if s[0] >= 0 else 0\n",
    "    return a\n",
    "\n",
    "simulate_policy(policy_Q2_3, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7a6e8a",
   "metadata": {},
   "source": [
    "This policy is better than the first one (of the 2nd question) but worst than the original one (from question 1). One interpretation can be that by shutting down the engine ($a_k=0$), we don't gain as much speed as we could (by for example decelerating only when $s_{k,0}\\geq 0$ and $s_{k,1}\\leq 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de3a98",
   "metadata": {},
   "source": [
    "___\n",
    "Question 3. We would like to compute the optimal policy which gets the car to the top of the hill as quickly as possible. As seen in class, the optimal policy can be found using dynamic programming, by implementing Value Iteration. Do you believe that it is possible in practice to do so for solving the problem at hand ? Justify your answer using precise quantitative arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30e8b5",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "Value Iteration runs in $O(|\\mathcal{A}||\\mathcal{S}|^2)$, with $\\mathcal{A},\\mathcal{S}$ the sets of possible actions and possible states. Althgough the number of possible actions is low, the number of states is very important and roughly proportional to $\\frac{1}{\\Delta}^2$, rendering Value Iteration impractical for small quantization sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89c4c0a",
   "metadata": {},
   "source": [
    "___\n",
    "Question 4. We now wish to find the best policy amongst a relatively small set of simple policies. We consider the set of policies such that $a_k$ solely depends on the signs of the position $s_{k,0}$ and speed $s_{k,0}$, so that\n",
    "$$\n",
    "a_k = f( \\text{sign}(s_{k,0}),\\text{sign}(s_{k,1}))\n",
    "$$\n",
    "where $f: \\{-1,1\\} \\to \\{-1,0,1\\}$. How many such policies are there ? For each of those policies, simulate the trajectory of the system, find the best one, and provide a physical argument why this policy is indeed the best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e13f3a",
   "metadata": {},
   "source": [
    "There are only $3^{(2\\times 2)} = 81$ possible policies in this policy set. Each policy can be represented as a 4-element vector giving the value of f for each sign configuration of $s_{k,0},s_{k,1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "916c3f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best policy is ((-1, 1, -1, 1)) with duration 424 steps\n"
     ]
    }
   ],
   "source": [
    "import itertools as iter\n",
    "\n",
    "def get_policy(vals):\n",
    "    '''\n",
    "    returns the policy function f given an value vector, containing the values of f for inputs [(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    '''\n",
    "    def f(s):\n",
    "        s0,s1 = int((np.sign(s[0])+1)/2), int((np.sign(s[1])+1)/2) # interpret the signs as bits\n",
    "        return vals[2*s0+s1]\n",
    "    \n",
    "    return f\n",
    "\n",
    "policies = iter.product([-1,0,1], repeat=4) # Enumerate all the possible policies\n",
    "\n",
    "T = 1000\n",
    "best = [np.NaN,np.NaN,np.NaN,np.NaN]\n",
    "best_score = T+1\n",
    "\n",
    "for p in policies:\n",
    "    policy = get_policy(p)\n",
    "    s_t = generate_episode(T,policy)\n",
    "    \n",
    "    for t in range(T):\n",
    "        if abs(s_t[t][0] - 1) < 1e-2 or s_t[t][0] >= 1:\n",
    "            if t<best_score:\n",
    "                best = p\n",
    "                best_score = t\n",
    "            break\n",
    "\n",
    "print('The best policy is ({}) with duration {} steps'.format(best,best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f8e8aa",
   "metadata": {},
   "source": [
    "This policy is expected since it accelerates as soon as the speed is positive, and reverses as soon as the speed is negative. Whenever we move backward, we take as much momentum as possible by accelerating backwards, and conversely accelarate forwards as much as we can as soon as we start moving forwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f6780e",
   "metadata": {},
   "source": [
    "___\n",
    "Question 5. Based on the question above, we now propose to apply policy gradient to the problem. We consider a set of parameterized policies, parameterized by a two dimensional vector $\\theta = (\\theta_0,\\theta_1)$, where the action probabilities are given as\n",
    "$$\n",
    "   \\mathbb{P}( a_t = 1| s_t) = \\phi( \\text{sign}(s_{k,0})\\theta_0 + \\theta_1 \\text{sign}(s_{k,1}))\n",
    "\\text{ and } \\mathbb{P}( a_t = -1| s_t) = 1-\\phi( \\text{sign}(s_{k,0})\\theta_0 + \\theta_1 \\text{sign}(s_{k,1}))\n",
    "$$\n",
    "where $\\phi$ is the sigmoidal function \n",
    "$$\n",
    "\\phi(x) = {1 \\over  1+e^{-x}}\n",
    "$$\n",
    "Compute the policy gradient estimate, and implement policy gradient descent as seen in class. How many iterations are necessary in order to obtain a good policy ?\n",
    "\n",
    "Note: you should be careful in how the initial value of $\\theta$ and the step size $\\eta$ are chosen, as those parameters impact the convergence speed and their values should be found by trial and error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d184684b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea6e93a0",
   "metadata": {},
   "source": [
    "___\n",
    "Question 6. We now wish to apply policy gradient to another set of parameterized policies, once again parameterized by a two dimensional vector $\\theta = (\\theta_0,\\theta_1)$ where the action probabilities are given as\n",
    "$$\n",
    "   \\mathbb{P}( a_t = 1| s_t) = \\phi( s^\\top \\theta)\n",
    "\\text{ and } \\mathbb{P}( a_t = -1| s_t) = 1-\\phi( s^\\top \\theta)\n",
    "$$\n",
    "where $\\phi$ is the sigmoidal function \n",
    "$$\n",
    "\\phi(x) = {1 \\over  1+e^{-x}}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "s^\\top \\theta= s_{k,0} \\theta_0 + \\theta_1 s_{k,1}\n",
    "$$\n",
    "which is the dot product between $s$ and $\\theta$ (i.e. the policy is linear).\n",
    "\n",
    "As in the previous question, compute the policy gradient estimate, and implement policy gradient descent as seen in class. How many iterations are necessary in order to obtain a good policy ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8893691d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef616dc7",
   "metadata": {},
   "source": [
    "___\n",
    "Question 7. We now wish to extend our results to a new model where the car is affected by a random wind. The effect of the wind is a random force so that the equation describing the system is modified as:\n",
    "$$\n",
    "{s_{k+1,0} - s_{k,0} \\over \\Delta} =  s_{k,1} \\text{     and       } {s_{k+1,1}-s_{k,1} \\over \\Delta}   =  { g s_{k,0} \\over 1 + (s_{k,0})^2 }  +  {K a_k \\over \\sqrt{1 + (s_{k,0})^2} } + L e_k\n",
    "$$\n",
    "where $e_k$ is i.i.d uniformly distributed in $\\{-1,+1\\}$ to model the random wind and $L$ is a coefficient describing the strength of the wind. We will consider $L=0.1$ here. Redo questions 4 and 5 for this new model, and comment on whether or not the results change (you should create a new  \"next_state\" subroutine in order to reflect this new model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d978a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
